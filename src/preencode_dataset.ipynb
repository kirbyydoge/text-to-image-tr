{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "import yaml\n",
    "import io\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models.vqgan import VQModel\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "DATA_PATH = \"../data/cc12m_sample/cc12m_tr.tsv\"\n",
    "OUT_PATH = \"../data/cc12m_sample/cc12m_tr_encoded.bin\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "PRETRAIN_DIR = \"pretrained\"\n",
    "MODEL_DIR = \"vqgan_f16_16384\"\n",
    "NUM_PATCHES = 256\n",
    "\n",
    "def load_config(config_path, display=False):\n",
    "    config = OmegaConf.load(config_path)\n",
    "    if display:\n",
    "        print(yaml.dump(OmegaConf.to_container(config)))\n",
    "    return config\n",
    "\n",
    "def load_vqgan(config, ckpt_path=None, is_gumbel=False):\n",
    "    model = VQModel(**config.model.params)\n",
    "    if ckpt_path is not None:\n",
    "        sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "        missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    return model.eval()\n",
    "\n",
    "def preprocess_vqgan(x):\n",
    "    x = 2.*x - 1.\n",
    "    return x\n",
    "\n",
    "def custom_to_pil(x):\n",
    "    x = x.detach().cpu()\n",
    "    x = torch.clamp(x, -1., 1.)\n",
    "    x = (x + 1.)/2.\n",
    "    x = x.permute(1,2,0).numpy()\n",
    "    x = (255*x).astype(np.uint8)\n",
    "    x = Image.fromarray(x)\n",
    "    if not x.mode == \"RGB\":\n",
    "        x = x.convert(\"RGB\")\n",
    "    return x\n",
    "\n",
    "def vqgan_encode(x, model):\n",
    "    with torch.no_grad():\n",
    "        z, _, [_, _, indices] = model.encode(x)\n",
    "        return z, indices\n",
    "\n",
    "def vqgan_decode(x, model):\n",
    "    with torch.no_grad():\n",
    "        return model.decode(x)\n",
    "\n",
    "def vqgan_reconstruct(x, model):\n",
    "    with torch.no_grad():\n",
    "        z, _, [_, _, indices] = model.encode(x)\n",
    "        xrec = model.decode(z)\n",
    "        return xrec\n",
    "\n",
    "def download_image(url, headers=HEADERS):\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    return Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "def preprocess(img, target_image_size=256):\n",
    "    s = min(img.size)\n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips\\vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "VQGAN Loaded.\n"
     ]
    }
   ],
   "source": [
    "cfg_vqgan = load_config(f\"../{PRETRAIN_DIR}/{MODEL_DIR}/configs/model.yaml\", display=False)\n",
    "model_vqgan = load_vqgan(cfg_vqgan, ckpt_path=f\"../{PRETRAIN_DIR}/{MODEL_DIR}/checkpoints/last.ckpt\").to(DEVICE)\n",
    "model_vqgan.eval()\n",
    "print(\"VQGAN Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oguzhan\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\transforms\\functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image = preprocess(download_image(\"https://www.abc.net.au/news/image/9329676-3x2-940x627.jpg\"), NUM_PATCHES).to(DEVICE)\n",
    "image_encode, image_tokens = vqgan_encode(image, model_vqgan)\n",
    "image_tokens = image_tokens.type(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def preencode(data_path, out_path, num_patches, device=\"cpu\", max_writes=-1, info_rate = 100):\n",
    "\twith open(data_path, \"r\", encoding=\"utf-8\") as src, open(out_path, \"wb\") as dest:\n",
    "\t\tdest.write(struct.pack(\"i\", num_patches))\n",
    "\t\tnum_writes = 0\n",
    "\t\tline_index = 0\n",
    "\t\tfor line in src:\n",
    "\t\t\tif num_writes == max_writes:\n",
    "\t\t\t\tbreak\n",
    "\t\t\turl, en, tr = line.strip().split(\"\\t\")\n",
    "\t\t\ttry:\n",
    "\t\t\t\timage = preprocess(download_image(url), num_patches).to(device)\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\t_, image_tokens = vqgan_encode(image, model_vqgan)\n",
    "\t\t\t\timage_tokens = image_tokens.flatten().type(torch.int32)\n",
    "\t\t\t\tassert len(image_tokens) == num_patches\n",
    "\t\t\t\tdest.write(struct.pack(\"i\", line_index))\n",
    "\t\t\t\tfor i in range(num_patches):\n",
    "\t\t\t\t\tdest.write(struct.pack(\"i\", image_tokens[i]))\n",
    "\t\t\t\tnum_writes += 1\n",
    "\t\t\t\tif num_writes % info_rate == 0:\n",
    "\t\t\t\t\tprint(f\"INFO: Total lines encoded - {num_writes}.\")\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"ERR: {e} at line {line_index}\")\n",
    "\t\t\tline_index += 1\n",
    "\n",
    "def load_tensor(f, patches):\n",
    "\tidx_bytes = f.read(4)\n",
    "\tif not idx_bytes:\n",
    "\t\treturn -1, None\n",
    "\tindex = struct.unpack(\"i\", idx_bytes)\n",
    "\tdata = torch.zeros(patches, dtype=torch.int32)\n",
    "\tfor i in range(patches):\n",
    "\t\tdata[i] = struct.unpack(\"i\", f.read(4))[0]\n",
    "\treturn index, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR: cannot identify image file <_io.BytesIO object at 0x000002179631FA40> at line 8\n",
      "INFO: 10 lines encoded.\n",
      "ERR: 403 Client Error: Forbidden for url: https://assets.nst.com.my/images/articles/18nt23sunway_1531880949.jpg at line 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Oguzhan\\Documents\\GitHub\\text-to-image-tr\\src\\preencode_dataset.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Oguzhan/Documents/GitHub/text-to-image-tr/src/preencode_dataset.ipynb#ch0000006?line=0'>1</a>\u001b[0m preencode(DATA_PATH, OUT_PATH, NUM_PATCHES, DEVICE)\n",
      "\u001b[1;32mc:\\Users\\Oguzhan\\Documents\\GitHub\\text-to-image-tr\\src\\preencode_dataset.ipynb Cell 4'\u001b[0m in \u001b[0;36mpreencode\u001b[1;34m(data_path, out_path, num_patches, device, max_writes, info_rate)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oguzhan/Documents/GitHub/text-to-image-tr/src/preencode_dataset.ipynb#ch0000003?line=10'>11</a>\u001b[0m url, en, tr \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oguzhan/Documents/GitHub/text-to-image-tr/src/preencode_dataset.ipynb#ch0000003?line=11'>12</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Oguzhan/Documents/GitHub/text-to-image-tr/src/preencode_dataset.ipynb#ch0000003?line=12'>13</a>\u001b[0m \timage \u001b[39m=\u001b[39m preprocess(download_image(url), num_patches)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oguzhan/Documents/GitHub/text-to-image-tr/src/preencode_dataset.ipynb#ch0000003?line=13'>14</a>\u001b[0m \t\u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oguzhan/Documents/GitHub/text-to-image-tr/src/preencode_dataset.ipynb#ch0000003?line=14'>15</a>\u001b[0m \t\t_, image_tokens \u001b[39m=\u001b[39m vqgan_encode(image, model_vqgan)\n",
      "\u001b[1;32mc:\\Users\\Oguzhan\\Documents\\GitHub\\text-to-image-tr\\src\\preencode_dataset.ipynb Cell 1'\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(img, target_image_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oguzhan/Documents/GitHub/text-to-image-tr/src/preencode_dataset.ipynb#ch0000000?line=68'>69</a>\u001b[0m r \u001b[39m=\u001b[39m target_image_size \u001b[39m/\u001b[39m s\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oguzhan/Documents/GitHub/text-to-image-tr/src/preencode_dataset.ipynb#ch0000000?line=69'>70</a>\u001b[0m s \u001b[39m=\u001b[39m (\u001b[39mround\u001b[39m(r \u001b[39m*\u001b[39m img\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m]), \u001b[39mround\u001b[39m(r \u001b[39m*\u001b[39m img\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m]))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Oguzhan/Documents/GitHub/text-to-image-tr/src/preencode_dataset.ipynb#ch0000000?line=70'>71</a>\u001b[0m img \u001b[39m=\u001b[39m TF\u001b[39m.\u001b[39;49mresize(img, s, interpolation\u001b[39m=\u001b[39;49mImage\u001b[39m.\u001b[39;49mLANCZOS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oguzhan/Documents/GitHub/text-to-image-tr/src/preencode_dataset.ipynb#ch0000000?line=71'>72</a>\u001b[0m img \u001b[39m=\u001b[39m TF\u001b[39m.\u001b[39mcenter_crop(img, output_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m [target_image_size])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oguzhan/Documents/GitHub/text-to-image-tr/src/preencode_dataset.ipynb#ch0000000?line=72'>73</a>\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(T\u001b[39m.\u001b[39mToTensor()(img), \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\transforms\\functional.py:375\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/torchvision/transforms/functional.py?line=372'>373</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/torchvision/transforms/functional.py?line=373'>374</a>\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/torchvision/transforms/functional.py?line=374'>375</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49msize, interpolation\u001b[39m=\u001b[39;49mpil_interpolation)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/torchvision/transforms/functional.py?line=376'>377</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39msize, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:228\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/torchvision/transforms/functional_pil.py?line=225'>226</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mresize((ow, oh), interpolation)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/torchvision/transforms/functional_pil.py?line=226'>227</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/torchvision/transforms/functional_pil.py?line=227'>228</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], interpolation)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\PIL\\Image.py:1922\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/PIL/Image.py?line=1913'>1914</a>\u001b[0m             \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mreduce(\u001b[39mself\u001b[39m, factor, box\u001b[39m=\u001b[39mreduce_box)\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/PIL/Image.py?line=1914'>1915</a>\u001b[0m         box \u001b[39m=\u001b[39m (\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/PIL/Image.py?line=1915'>1916</a>\u001b[0m             (box[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/PIL/Image.py?line=1916'>1917</a>\u001b[0m             (box[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/PIL/Image.py?line=1917'>1918</a>\u001b[0m             (box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/PIL/Image.py?line=1918'>1919</a>\u001b[0m             (box[\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/PIL/Image.py?line=1919'>1920</a>\u001b[0m         )\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python38/lib/site-packages/PIL/Image.py?line=1921'>1922</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mresize(size, resample, box))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preencode(DATA_PATH, OUT_PATH, NUM_PATCHES, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "116fc0531d454bbdeaf23f70d07b0d49aee0978cb9b9ebe8756766f8c910747e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
